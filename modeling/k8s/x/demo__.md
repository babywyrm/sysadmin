
------------------------------------------------------------------------------
1) Repository Layout (project-x/)
------------------------------------------------------------------------------
```
â”œâ”€â”€ .env.example
â”œâ”€â”€ .github
â”‚   â””â”€â”€ workflows
â”‚       â”œâ”€â”€ ci.yaml
â”‚       â””â”€â”€ cd.yaml
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ docs
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ flow-diagram.mmd
â”‚   â””â”€â”€ ENV.md
â”œâ”€â”€ infra
â”‚   â”œâ”€â”€ terraform
â”‚   â”‚   â”œâ”€â”€ gke-cluster.tf
â”‚   â”‚   â””â”€â”€ redis-cluster.tf
â”‚   â””â”€â”€ kustomize
â”‚       â”œâ”€â”€ base
â”‚       â”‚   â”œâ”€â”€ namespace.yaml
â”‚       â”‚   â””â”€â”€ common-labels.yaml
â”‚       â”œâ”€â”€ overlays
â”‚       â”‚   â”œâ”€â”€ dev
â”‚       â”‚   â””â”€â”€ prod
â”‚       â””â”€â”€ kustomization.yaml
â”œâ”€â”€ config
â”‚   â”œâ”€â”€ spire
â”‚   â”‚   â”œâ”€â”€ server-config.hcl
â”‚   â”‚   â””â”€â”€ agent-config.hcl
â”‚   â”œâ”€â”€ istio
â”‚   â”‚   â”œâ”€â”€ virtual-service-challenge.yaml
â”‚   â”‚   â””â”€â”€ authorization-policy.yaml
â”‚   â”œâ”€â”€ opa
â”‚   â”‚   â”œâ”€â”€ templates
â”‚   â”‚   â”‚   â”œâ”€â”€ signed-images-template.yaml
â”‚   â”‚   â”‚   â””â”€â”€ resource-limits-template.yaml
â”‚   â”‚   â””â”€â”€ constraints
â”‚   â”‚       â”œâ”€â”€ signed-images.yaml
â”‚   â”‚       â””â”€â”€ resource-limits.yaml
â”‚   â””â”€â”€ ambassador
â”‚       â”œâ”€â”€ authservice.yaml
â”‚       â”œâ”€â”€ mapping-login.yaml
â”‚       â””â”€â”€ mapping-challenges.yaml
â”œâ”€â”€ apps
â”‚   â”œâ”€â”€ auth-service
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ main.go
â”‚   â”‚   â””â”€â”€ config.yaml
â”‚   â””â”€â”€ challenge-controller
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â”œâ”€â”€ main.go
â”‚       â””â”€â”€ config.yaml
â””â”€â”€ charts
    â”œâ”€â”€ spire
    â”œâ”€â”€ istio-config
    â”œâ”€â”€ opa
    â””â”€â”€ ambassador
```
------------------------------------------------------------------------------
2) .env.example
------------------------------------------------------------------------------
```
# project-x environment
PROJECT_X_DOMAIN=project-x.example.com
TRUST_DOMAIN=project-x.local
JWT_PRIVATE_KEY=/secrets/jwt/private.key
JWT_PUBLIC_KEY=/secrets/jwt/public.key
MONGO_URI=mongodb+srv://user:pass@cluster0.mongodb.net/projectx
REDIS_ADDR=redis-cluster.project-x.svc.cluster.local:6379
IMAGE_REGISTRY=registry.project-x.local
SESSION_TTL=24h
CHALLENGE_TTL=2h
OPA_DATA_PATH=/opa/data
SPIRE_SERVER_ADDR=spire-server.project-x.svc.cluster.local:8081
SPIRE_AGENT_SOCKET=/run/spire/sockets/agent.sock
ISTIO_NAMESPACE=istio-system
```


------------------------------------------------------------------------------
3) config/spire/server-config.hcl
------------------------------------------------------------------------------
```
server {
  bind_address = "0.0.0.0"
  bind_port    = "8081"
  trust_domain = "project-x.local"
  data_dir     = "/opt/spire/data"
  ca_subject {
    country      = ["US"]
    organization = ["Project X"]
    common_name  = "Project-X SPIRE Server"
  }
}

plugin "datastore/sql" {
  plugin_data {
    database_type     = "postgres"
    connection_string = "postgresql://spire:spirepass@postgres-svc:5432/spire?sslmode=disable"
  }
}

plugin "node_attestor" {
  plugin_data {
    plugin_name = "k8s_sat"
    # allow SPIRE agent service account
    service_account_allow_list = ["spire-system:spire-agent"]
  }
}

plugin "workload_attestor" {
  plugin_data {
    plugin_name = "k8s"
    node_name_env       = "K8S_NODE_NAME"
    required_annotations = ["project-x/challenge-id", "project-x/user-id", "project-x/tier"]
  }
}
```

------------------------------------------------------------------------------
4) config/spire/agent-config.hcl
------------------------------------------------------------------------------
```
agent {
  data_dir       = "/opt/spire/data"
  log_level      = "INFO"
  server_address = "spire-server.project-x.svc.cluster.local"
  server_port    = "8081"
  trust_domain   = "project-x.local"
  socket_path    = "/run/spire/sockets/agent.sock"
}

plugin "node_attestor" {
  plugin_data {
    plugin_name = "k8s_sat"
    cluster     = "project-x-cluster"
    token_path  = "/var/run/secrets/tokens/spire-agent"
  }
}

plugin "workload_attestor" {
  plugin_data {
    plugin_name             = "k8s"
    kubelet_read_only_port  = 10255
    required_annotations    = ["project-x/challenge-id","project-x/user-id","project-x/tier"]
    verify_image_signature  = true
    cosign_public_key_path  = "/opt/spire/conf/cosign.pub"
  }
}
```

------------------------------------------------------------------------------
5) config/istio/virtual-service-challenge.yaml
------------------------------------------------------------------------------
```
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: challenge-{{CHALLENGE_ID}}
  namespace: project-x-challenges
spec:
  hosts:
  - "{{CHALLENGE_ID}}.{{PROJECT_X_DOMAIN}}"
  gateways:
  - project-x-gateway
  http:
  - match:
    - headers:
        authorization:
          regex: ".*challenge_id:{{CHALLENGE_ID}}.*"
    route:
    - destination:
        host: "{{CHALLENGE_ID}}.project-x-challenges.svc.cluster.local"
        port:
          number: 8080
    timeout: 300s
    retries:
      attempts: 3
      perTryTimeout: 30s
```
------------------------------------------------------------------------------
6) config/istio/authorization-policy.yaml
------------------------------------------------------------------------------
```
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: challenge-{{CHALLENGE_ID}}-authz
  namespace: project-x-challenges
spec:
  selector:
    matchLabels:
      project-x/challenge-id: "{{CHALLENGE_ID}}"
  action: ALLOW
  rules:
  - from:
    - source:
        principals: ["cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account"]
    when:
    - key: "request.auth.claims.challenge_id"
      values: ["{{CHALLENGE_ID}}"]
    - key: "request.auth.claims.user_id"
      values: ["{{USER_ID}}"]
```

------------------------------------------------------------------------------
7) config/opa/templates/signed-images-template.yaml
------------------------------------------------------------------------------
```
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: signedimagesonly
spec:
  crd:
    spec:
      names:
        kind: SignedImagesOnly
      validation:
        properties:
          allowedRegistries:
            type: array
            items: {type: string}
          cosignPublicKey:
            type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package signedimagesonly
        violation[{"msg": msg}] {
          input.review.object.kind == "Pod"
          image := input.review.object.spec.containers[_].image
          not startswith(image, input.parameters.allowedRegistries[_])
          msg := sprintf("Image %v not in allowed registries", [image])
        }
        violation[{"msg": msg}] {
          input.review.object.kind == "Pod"
          image := input.review.object.spec.containers[_].image
          not image_has_valid_signature(image, input.parameters.cosignPublicKey)
          msg := sprintf("Image %v missing valid cosign signature", [image])
        }
```
------------------------------------------------------------------------------
8) config/opa/constraints/signed-images.yaml
------------------------------------------------------------------------------
```
apiVersion: config.gatekeeper.sh/v1alpha1
kind: SignedImagesOnly
metadata:
  name: projectx-signed-images
spec:
  allowedRegistries:
    - "{{IMAGE_REGISTRY}}/"
  cosignPublicKey: |
    -----BEGIN PUBLIC KEY-----
    {{COSIGN_PUBKEY}}
    -----END PUBLIC KEY-----
```
------------------------------------------------------------------------------
9) config/opa/templates/resource-limits-template.yaml
------------------------------------------------------------------------------
```
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: projectxresourcelimits
spec:
  crd:
    spec:
      names:
        kind: ProjectXResourceLimits
      validation:
        properties:
          tierLimits:
            type: object
            additionalProperties:
              type: object
              properties:
                maxChallenges: {type: integer}
                maxCPU: {type: string}
                maxMemory: {type: string}
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package projectxresourcelimits
        violation[{"msg": msg}] {
          pod := input.review.object
          tier := pod.metadata.annotations["project-x/tier"]
          user := pod.metadata.annotations["project-x/user-id"]
          # count existing
          existing := count([
            p | p := data.inventory.namespace["project-x-challenges"].v1.Pod[_];
                  p.metadata.annotations["project-x/user-id"] == user;
                  p.metadata.annotations["project-x/tier"] == tier
          ])
          limits := input.parameters.tierLimits[tier]
          existing >= limits.maxChallenges
          msg := sprintf("User %v has too many challenges for tier %v", [user,tier])
        }
```
------------------------------------------------------------------------------
10) config/opa/constraints/resource-limits.yaml
------------------------------------------------------------------------------
```
apiVersion: config.gatekeeper.sh/v1alpha1
kind: ProjectXResourceLimits
metadata:
  name: projectx-tier-limits
spec:
  tierLimits:
    tier-1:
      maxChallenges: 3
      maxCPU: "500m"
      maxMemory: "1Gi"
    tier-2:
      maxChallenges: 5
      maxCPU: "1000m"
      maxMemory: "2Gi"
    tier-3:
      maxChallenges: 10
      maxCPU: "2000m"
      maxMemory: "4Gi"
```
------------------------------------------------------------------------------
11) config/ambassador/authservice.yaml
------------------------------------------------------------------------------
```
apiVersion: getambassador.io/v3alpha1
kind: AuthService
metadata:
  name: projectx-auth-service
  namespace: ambassador
spec:
  auth_service: "project-x-auth.project-x.svc.cluster.local:8080"
  path_prefix: "/auth"
  timeout_ms: 5000
  allowed_request_headers:
  - "content-type"
  - "authorization"
  - "x-session-id"
  allowed_authorization_headers:
  - "x-user-id"
  - "x-user-tiers"
  - "x-session-id"
```
------------------------------------------------------------------------------
12) config/ambassador/mapping-login.yaml
------------------------------------------------------------------------------
```
apiVersion: getambassador.io/v3alpha1
kind: Mapping
metadata:
  name: auth-login
  namespace: ambassador
spec:
  hostname: project-x.example.com
  prefix: /auth/login
  service: project-x-auth.project-x.svc.cluster.local:8080
  timeout_ms: 10000
  cors:
    origins: ["https://project-x.example.com"]
    methods: ["POST","OPTIONS"]
    headers: ["Content-Type","Authorization"]
    credentials: true
```

------------------------------------------------------------------------------
13) config/ambassador/mapping-challenges.yaml
------------------------------------------------------------------------------
```
apiVersion: getambassador.io/v3alpha1
kind: Mapping
metadata:
  name: challenge-api
  namespace: ambassador
spec:
  hostname: project-x.example.com
  prefix: /api/challenges
  service: project-x-challenge-api.project-x.svc.cluster.local:8080
  timeout_ms: 30000
  auth_service: "projectx-auth-service"
  filters:
  - name: jwt-validation
    jwt:
      issuer: "project-x.example.com"
      audience: "project-x"
      jwksURI: "https://project-x.example.com/.well-known/jwks.json"
  - name: rate-limiting
    rateLimit:
      domain: project-x
      rates:
      - unit: minute
        requestsPerUnit: 10
      descriptors:
      - key: "user_id"
        value: "%REQ(x-user-id)%"
```
------------------------------------------------------------------------------
14) apps/auth-service/main.go  (skeleton)
------------------------------------------------------------------------------
package main

import (
    "context"
    "log"
    "net/http"
    "time"

    "github.com/golang-jwt/jwt/v4"
    "go.mongodb.org/mongo-driver/mongo"
    "github.com/go-redis/redis/v8"
)

func main() {
    // load config, connect Mongo, Redis, load RSA keys
    // expose /auth/login, /auth/logout, /auth/validate
    log.Println("Project-X Auth Service startingâ€¦")
    http.ListenAndServe(":8080", nil)
}

// handlers: Login, ValidateJWT, CreateChallengeToken
// use JWTClaims { UserID, Email, Subscriptions, TierLimits, SessionID }
```

------------------------------------------------------------------------------
15) apps/challenge-controller/main.go  (skeleton)
------------------------------------------------------------------------------
package main

import (
    "context"
    "log"
    "time"

    "k8s.io/client-go/kubernetes"
)

func main() {
    // load config, connect to K8s, SPIRE, OPA, Istio CRD client
    // start HTTP server on :8080 for /challenges endpoint
    // start background cleanup ticker
    log.Println("Project-X Challenge Controller startingâ€¦")
    select {}
}

// SpawnChallenge ->
// 1. validate via OPA gRPC  
// 2. gen SPIFFE ID  
// 3. k8s.Deploy Deployment+Service  
// 4. SPIRE CreateEntry  
// 5. Istio CRDs (VirtualService, AuthorizationPolicy)
// 6. return {ID, Endpoint, ExpiresAt}

```

------------------------------------------------------------------------------
16) docs/architecture.md (excerpt)
------------------------------------------------------------------------------
# Project-X Architecture

- **Auth Service**: MongoDB + Redis session store â†’ JWT minting  
- **Ambassador Gateway**: JWT validation, rate limiting, CORS, routing  
- **OPA/Gatekeeper**: Admission policies for resource & image security  
- **SPIRE/SPIFFE**: Workload identity (SVIDs) with k8s attestors  
- **Istio Service Mesh**: mTLS, routing, AuthorizationPolicy  
- **Challenge Controller**: on-demand challenge pod lifecycle  
- **Kubernetes Namespaces**: tier-1, tier-2, tier-3, infra  
- **Infra**: GKE/EKS (Terraform), Redis, MongoDB Atlas  

See `flow-diagram.mmd` for sequence diagrams & ASCII art  

------------------------------------------------------------------------------
17) docs/flow-diagram.mmd (excerpt)
------------------------------------------------------------------------------
```mermaid
sequenceDiagram
    participant U as User
    participant AM as Ambassador
    participant AU as Auth Service
    participant MO as MongoDB
    participant RE as Redis
    participant CH as Challenge API
    participant CC as Controller
    participant KS as Kubernetes
    participant SP as SPIRE
    participant IS as Istio
    participant OP as OPA

    U->>AM: POST /auth/login
    AM->>AU: validate creds â†’ JWT + session
    AU->>MO: user lookup
    AU->>RE: store session
    AU->>AM: return JWT
    U->>AM: POST /api/challenges + JWT
    AM->>RE: session check
    AM->>CH: forward request
    CH->>OP: pre-admission check
    CH->>KS: create Deployment/Service
    KS->>OP: admit Pod via Gatekeeper
    KS->>SP: agent attest â†’ SVID issued
    KS->>IS: sidecar mTLS injected
    CH->>SP: register entry
    CH->>IS: apply VS & AuthZ
    AM->>U: return challenge URL
    U->>AM: GET challenge URL + token
    AM->>IS: route â†’ Envoy â†’ Pod
```

------------------------------------------------------------------------------
18) README.md (excerpt)
------------------------------------------------------------------------------
# Project-X CTF Platform Blueprint

## ðŸ›  Technologies
- Kubernetes (GKE/EKS)
- SPIFFE/SPIRE for workload identity
- Istio for mTLS & routing
- OPA/Gatekeeper for admission policies
- Ambassador for edge JWT auth & rate-limiting
- MongoDB Atlas, Redis for state
- Go microservices for Auth & Controller

## ðŸš€ Getting Started
1. `cp .env.example .env` & fill in your secrets  
2. Deploy infra:  
   - `terraform apply infra/terraform`  
   - `kubectl apply -k infra/kustomize/overlays/dev`
3. Install SPIRE via Helm: `helm install spire charts/spire`  
4. Apply SPIRE server/agent configs: `kubectl apply -f config/spire`  
5. Install OPA & Gatekeeper: `helm install gatekeeper charts/opa`  
6. Apply OPA templates & constraints: `kubectl apply -f config/opa`  
7. Install Istio core + CRDs: `istioctl install --set profile=demo`  
8. Apply Istio config: `kubectl apply -f config/istio`  
9. Install Ambassador: `helm install ambassador charts/ambassador`  
10. Apply Ambassador mappings: `kubectl apply -f config/ambassador`  
11. Build & push auth & controller images to `IMAGE_REGISTRY`  
12. Deploy apps:  
    - `kubectl apply -f apps/auth-service`  
    - `kubectl apply -f apps/challenge-controller`

## ðŸ“š Docs
- `docs/architecture.md` â€“ high-level design  
- `docs/flow-diagram.mmd` â€“ sequence diagrams  
- `docs/ENV.md` â€“ detailed env vars

## ðŸ“ˆ Scaling
- **Nodes**: autoscaled via GKE/EKS  
- **Control Plane**: SPIRE (10 replicas), Controller (50), Istiod (20)  
- **DB**: MongoDB Atlas M60 auto-scale  
- **Cache**: Redis cluster with 6 shards  
- **WAN**: Ambassador + Istio ingress  

## ðŸ›¡ Security
- Zero-trust via SPIFFE/SPIRE  
- Automatic mTLS in Istio  
- Image signing with Cosign  
- Admission policies (OPA)  
- Per-challenge SVIDs & scoped JWTs  
- Namespace & network policy isolation  

---


# Project-X Phased Rollout Plan

This document outlines a 6-phase rollout for Project-X: from core Auth through full zero-trust challenge hosting and scaling to 50K users.

--------------------------------------------------------------------------------
Phase 1 â€“ Core Authentication & Session Management
--------------------------------------------------------------------------------
Objective
â€¢ Stand up user login, JWT minting, session store  
â€¢ Validate credentials against MongoDB, track sessions in Redis  

Key Deliverables
â€¢ `.env` populated (`MONGO_URI`, `REDIS_ADDR`, `JWT_PRIVATE_KEY`, etc.)  
â€¢ Ambassador mapping: `config/ambassador/mapping-login.yaml`  
â€¢ Auth-Service code & Dockerfile in `apps/auth-service/`  
â€¢ MongoDB Atlas + Redis Cluster infra (Terraform in `infra/terraform/`)  
â€¢ CI/CD pipelines: `.github/workflows/ci.yaml` / `cd.yaml`

Success Criteria
â€¢ 10K logins/minute, P95 < 2s  
â€¢ 24h session TTL in Redis, secure JWT RS256  
â€¢ End-to-end login â†’ â€œ200 OK + Set-Cookie + JWTâ€

--------------------------------------------------------------------------------
Phase 2 â€“ Challenge API & Admission Policies
--------------------------------------------------------------------------------
Objective
â€¢ Expose `/api/challenges` endpoint  
â€¢ Validate user claims, enforce tier quotas and image signing  

Key Deliverables
â€¢ Ambassador mapping: `config/ambassador/mapping-challenges.yaml`  
â€¢ Gatekeeper templates & constraints:  
  - `config/opa/templates/signed-images-template.yaml`  
  - `config/opa/constraints/signed-images.yaml`  
  - `config/opa/templates/resource-limits-template.yaml`  
  - `config/opa/constraints/resource-limits.yaml`  
â€¢ Challenge-Controller skeleton in `apps/challenge-controller/`  
â€¢ Kustomize overlays for `dev`/`prod` under `infra/kustomize/`

Success Criteria
â€¢ OPA rejects unauthorized tiers or unsigned images  
â€¢ `/api/challenges` â†’ â€œ202 Acceptedâ€ for valid requests  
â€¢ Rate-limit 10 req/min/user enforced at Ambassador

--------------------------------------------------------------------------------
Phase 3 â€“ SPIRE & Istio Service Mesh
--------------------------------------------------------------------------------
Objective
â€¢ Issue SPIFFE SVIDs for workloads, enforce mTLS  
â€¢ Route traffic via Istio VirtualServices & AuthorizationPolicies  

Key Deliverables
â€¢ SPIRE Server config: `config/spire/server-config.hcl`  
â€¢ SPIRE Agent config: `config/spire/agent-config.hcl`  
â€¢ Istio install + CRDs in `config/istio/`:
  - `virtual-service-challenge.yaml`  
  - `authorization-policy.yaml`  
â€¢ Helm charts in `charts/spire` and `charts/istio-config`

Success Criteria
â€¢ Pod-to-pod communication mTLS STRICT by default  
â€¢ SPIRE issues per-challenge certificates with correct `challenge_id` claim  
â€¢ Istio routes `*.project-x.example.com` â†’ correct namespace/pod

--------------------------------------------------------------------------------
Phase 4 â€“ End-to-End Challenge Lifecycle
--------------------------------------------------------------------------------
Objective
â€¢ Wire up full flow: login â†’ challenge spawn â†’ user access â†’ cleanup  

Key Deliverables
â€¢ Complete Challenge Controller logic (spawn, register SPIRE entry, Istio CRDs, cleanup)  
  in `apps/challenge-controller/main.go` & `config.toml`  
â€¢ Ambassador AuthService for scoped challenge tokens  
â€¢ Flow diagrams in `docs/flow-diagram.mmd`  
â€¢ End-to-end smoke tests in `.github/workflows/ci.yaml`

Success Criteria
â€¢ P95 â€œspawn+readyâ€ < 45 s, P99 < 60 s  
â€¢ 100K active challenges/day supported in dev cluster  
â€¢ Automatic cleanup of expired pods & SPIRE entries

--------------------------------------------------------------------------------
Phase 5 â€“ Performance Tuning & Security Hardening
--------------------------------------------------------------------------------
Objective
â€¢ Optimize autoscaling, CNI policies, and add OPA/NetworkPolicy enforcement  

Key Deliverables
â€¢ HPA & Cluster Autoscaler settings in `infra/kustomize/overlays/prod`  
â€¢ Cilium/Calico NetworkPolicies for per-challenge isolation  
â€¢ Optional: Falco rules, eBPF monitor (deferred to v2)  
â€¢ SPIRE performance tweaks: caching, increased server/agent replicas

Success Criteria
â€¢ Node utilization: 70â€“90% under peak  
â€¢ <10 ms p99 policy decision latency (OPA)  
â€¢ Zero â€œcross-tenantâ€ network flows in Calico/Cilium telemetry

--------------------------------------------------------------------------------
Phase 6 â€“ Production Launch & Monitoring
--------------------------------------------------------------------------------
Objective
â€¢ Deploy to prod, enable full observability, finalize SLO/SLA  

Key Deliverables
â€¢ Prometheus + Grafana Dashboards (ServiceMonitor & PromRule in `config/monitoring/`)  
â€¢ Alertmanager rules for health, SVID expiry, policy violations  
â€¢ SLO document & runbook in `docs/`  
â€¢ CI/CD gated deploy to `overlays/prod`  

Success Criteria
â€¢ 99.9% platform uptime, 99.5% challenge availability  
â€¢ <5 min recovery time on node/controller failures  
â€¢ Real-time alerting on security or performance incidents  

--------------------------------------------------------------------------------
Recovery & Iteration
--------------------------------------------------------------------------------
â€¢ Post-mortem cadence after any P1/P0 incident  
â€¢ Quarterly security review: rotate keys (JWT, SPIRE), update OPA policies  
â€¢ Monthly performance tuning: adjust HPA thresholds, instance types  

---

**Next Steps:**  
1. âœ”ï¸ Kick off Phase 1 immediately (Auth + Redis + MongoDB)  
2. ðŸ“… Review Phase 2 in 2 weeks, adjust scope based on learnings  
3. ðŸš€ Execute Phase 3â€“4 by end of Q3, begin scaling experiments  
4. ðŸ“Š Prepare executive dashboards for Phase 5 metrics  
5. ðŸŽ‰ Go-live Phase 6 for full public launch in Q4  



